{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n作者：英俊\\nQQ:2227495940\\n邮箱 同上\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "作者：英俊\n",
    "QQ:2227495940\n",
    "邮箱 同上\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>六一出生的？好讽刺…… //@祭春姬:他爸爸是外星人吧 //@面孔小高:现在的孩子都怎么了 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>听过一场！笑死了昂，一听茄子脱口秀，从此节操是路人！[嘻嘻] //@中国梦网官微:@Penc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>说不出话来[泪] //@央广郭亮：//@戴围脖的Luc:试运营不能搭载乘客的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>疼死我了，一下汗就出来了！扎我一针居然说血不够！原伤口又狠狠的给了一下！十指连心呢！我恨这个...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>凭啥不让发?!支持! //@臭丫头妈:协警叔叔过来挑理，不让发宣传单页。姐只好不发了。结果周...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  六一出生的？好讽刺…… //@祭春姬:他爸爸是外星人吧 //@面孔小高:现在的孩子都怎么了 ...\n",
       "1      1  听过一场！笑死了昂，一听茄子脱口秀，从此节操是路人！[嘻嘻] //@中国梦网官微:@Penc...\n",
       "2      0             说不出话来[泪] //@央广郭亮：//@戴围脖的Luc:试运营不能搭载乘客的\n",
       "3      0  疼死我了，一下汗就出来了！扎我一针居然说血不够！原伤口又狠狠的给了一下！十指连心呢！我恨这个...\n",
       "4      0  凭啥不让发?!支持! //@臭丫头妈:协警叔叔过来挑理，不让发宣传单页。姐只好不发了。结果周..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train=pd.read_csv(r'C:\\Users\\Administrator\\.fastNLP\\dataset\\WeiboSenti100k\\train.tsv', sep='\\t')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>买灯难，难于上青天。[悲伤]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@马勒别墅 给新人送上深深的祝福 要幸福哦~ Mia[可爱]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>//@广播:[赞][哈哈]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#快讯#【高速交通广播 广西田东面包车翻下山崖，1死3伤】今天中午，一辆载有5人的面包车，在...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>三毛先生你说的好有道理耶！[鼓掌]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0                                     买灯难，难于上青天。[悲伤]\n",
       "1      1                     @马勒别墅 给新人送上深深的祝福 要幸福哦~ Mia[可爱]\n",
       "2      1                                      //@广播:[赞][哈哈]\n",
       "3      0  #快讯#【高速交通广播 广西田东面包车翻下山崖，1死3伤】今天中午，一辆载有5人的面包车，在...\n",
       "4      1                                  三毛先生你说的好有道理耶！[鼓掌]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_dev=pd.read_csv(r'C:\\Users\\Administrator\\.fastNLP\\dataset\\WeiboSenti100k\\dev.tsv', sep='\\t')\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>就是累了，比舒小姐要早就想离开。[可爱] //@任卫新:受到舒淇影响了[泪]//@赖宝: 生...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>今天紫外线强度太强了，大家出门注意防晒，[泪][泪][泪]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>~~ //@lichgo://@雪虹坚持跳绳:小智~~！！ //@Yi淳爱动画:[嘻嘻] 小...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>有一个小姑娘，她有两部ipad，担心我买她的ipad，就把一部送人了，就这样还不放心，情急之...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>感动[泪][泪]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  就是累了，比舒小姐要早就想离开。[可爱] //@任卫新:受到舒淇影响了[泪]//@赖宝: 生...\n",
       "1      0                      今天紫外线强度太强了，大家出门注意防晒，[泪][泪][泪]\n",
       "2      0  ~~ //@lichgo://@雪虹坚持跳绳:小智~~！！ //@Yi淳爱动画:[嘻嘻] 小...\n",
       "3      0  有一个小姑娘，她有两部ipad，担心我买她的ipad，就把一部送人了，就这样还不放心，情急之...\n",
       "4      0                                           感动[泪][泪]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_test=pd.read_csv(r'C:\\Users\\Administrator\\.fastNLP\\dataset\\WeiboSenti100k\\test.tsv', sep='\\t')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99958</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99959</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99960</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99961</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99962</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99963</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99964</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99965</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99966</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99967</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99968</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99969</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99988 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label   text\n",
       "0      False  False\n",
       "1      False  False\n",
       "2      False  False\n",
       "3      False  False\n",
       "4      False  False\n",
       "5      False  False\n",
       "6      False  False\n",
       "7      False  False\n",
       "8      False  False\n",
       "9      False  False\n",
       "10     False  False\n",
       "11     False  False\n",
       "12     False  False\n",
       "13     False  False\n",
       "14     False  False\n",
       "15     False  False\n",
       "16     False  False\n",
       "17     False  False\n",
       "18     False  False\n",
       "19     False  False\n",
       "20     False  False\n",
       "21     False  False\n",
       "22     False  False\n",
       "23     False  False\n",
       "24     False  False\n",
       "25     False  False\n",
       "26     False  False\n",
       "27     False  False\n",
       "28     False  False\n",
       "29     False  False\n",
       "...      ...    ...\n",
       "99958  False  False\n",
       "99959  False  False\n",
       "99960  False  False\n",
       "99961  False  False\n",
       "99962  False  False\n",
       "99963  False  False\n",
       "99964  False  False\n",
       "99965  False  False\n",
       "99966  False  False\n",
       "99967  False  False\n",
       "99968  False  False\n",
       "99969  False  False\n",
       "99970  False  False\n",
       "99971  False  False\n",
       "99972  False  False\n",
       "99973  False  False\n",
       "99974  False  False\n",
       "99975  False  False\n",
       "99976  False  False\n",
       "99977  False  False\n",
       "99978  False  False\n",
       "99979  False  False\n",
       "99980  False  False\n",
       "99981  False  False\n",
       "99982  False  False\n",
       "99983  False  False\n",
       "99984  False  False\n",
       "99985  False  False\n",
       "99986  False  False\n",
       "99987  False  False\n",
       "\n",
       "[99988 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>六一出生的？好讽刺…… //@祭春姬:他爸爸是外星人吧 //@面孔小高:现在的孩子都怎么了 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>听过一场！笑死了昂，一听茄子脱口秀，从此节操是路人！[嘻嘻] //@中国梦网官微:@Penc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>说不出话来[泪] //@央广郭亮：//@戴围脖的Luc:试运营不能搭载乘客的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>疼死我了，一下汗就出来了！扎我一针居然说血不够！原伤口又狠狠的给了一下！十指连心呢！我恨这个...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>凭啥不让发?!支持! //@臭丫头妈:协警叔叔过来挑理，不让发宣传单页。姐只好不发了。结果周...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  六一出生的？好讽刺…… //@祭春姬:他爸爸是外星人吧 //@面孔小高:现在的孩子都怎么了 ...\n",
       "1      1  听过一场！笑死了昂，一听茄子脱口秀，从此节操是路人！[嘻嘻] //@中国梦网官微:@Penc...\n",
       "2      0             说不出话来[泪] //@央广郭亮：//@戴围脖的Luc:试运营不能搭载乘客的\n",
       "3      0  疼死我了，一下汗就出来了！扎我一针居然说血不够！原伤口又狠狠的给了一下！十指连心呢！我恨这个...\n",
       "4      0  凭啥不让发?!支持! //@臭丫头妈:协警叔叔过来挑理，不让发宣传单页。姐只好不发了。结果周..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all=pd.concat([df_train,df_test],ignore_index=True)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109988 entries, 0 to 109987\n",
      "Data columns (total 2 columns):\n",
      "label    109988 non-null int64\n",
      "text     109988 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all1=pd.DataFrame({\"raw_words\":df_all['text'],\"target\":df_all['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all1.to_csv('df_train.txt',sep='\\t', index=False,header=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_words</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>六一出生的？好讽刺…… //@祭春姬:他爸爸是外星人吧 //@面孔小高:现在的孩子都怎么了 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>听过一场！笑死了昂，一听茄子脱口秀，从此节操是路人！[嘻嘻] //@中国梦网官微:@Penc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>说不出话来[泪] //@央广郭亮：//@戴围脖的Luc:试运营不能搭载乘客的</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>疼死我了，一下汗就出来了！扎我一针居然说血不够！原伤口又狠狠的给了一下！十指连心呢！我恨这个...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>凭啥不让发?!支持! //@臭丫头妈:协警叔叔过来挑理，不让发宣传单页。姐只好不发了。结果周...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_words  target\n",
       "0  六一出生的？好讽刺…… //@祭春姬:他爸爸是外星人吧 //@面孔小高:现在的孩子都怎么了 ...       0\n",
       "1  听过一场！笑死了昂，一听茄子脱口秀，从此节操是路人！[嘻嘻] //@中国梦网官微:@Penc...       1\n",
       "2             说不出话来[泪] //@央广郭亮：//@戴围脖的Luc:试运营不能搭载乘客的       0\n",
       "3  疼死我了，一下汗就出来了！扎我一针居然说血不够！原伤口又狠狠的给了一下！十指连心呢！我恨这个...       0\n",
       "4  凭啥不让发?!支持! //@臭丫头妈:协警叔叔过来挑理，不让发宣传单页。姐只好不发了。结果周...       0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>就是累了，比舒小姐要早就想离开。[可爱] //@任卫新:受到舒淇影响了[泪]//@赖宝: 生...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>今天紫外线强度太强了，大家出门注意防晒，[泪][泪][泪]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>~~ //@lichgo://@雪虹坚持跳绳:小智~~！！ //@Yi淳爱动画:[嘻嘻] 小...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>有一个小姑娘，她有两部ipad，担心我买她的ipad，就把一部送人了，就这样还不放心，情急之...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>感动[泪][泪]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_words\n",
       "0  就是累了，比舒小姐要早就想离开。[可爱] //@任卫新:受到舒淇影响了[泪]//@赖宝: 生...\n",
       "1                      今天紫外线强度太强了，大家出门注意防晒，[泪][泪][泪]\n",
       "2  ~~ //@lichgo://@雪虹坚持跳绳:小智~~！！ //@Yi淳爱动画:[嘻嘻] 小...\n",
       "3  有一个小姑娘，她有两部ipad，担心我买她的ipad，就把一部送人了，就这样还不放心，情急之...\n",
       "4                                           感动[泪][泪]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testlist=df_test['label']\n",
    "# # trainlist=int(trainlist)\n",
    "# testlist=[int(i) for i in testlist]\n",
    "# ,'target':df_test['label']\n",
    "df_test1=pd.DataFrame({'raw_words':df_test['text']})\n",
    "df_test1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test1.to_csv('df_test.txt',sep='\\t', index=False,header=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入Pytorch包\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from fastNLP.io.loader import CSVLoader\n",
    "\n",
    "dataset_loader = CSVLoader(headers=('raw_words','target'), sep='\\t')\n",
    "testset_loader = CSVLoader( headers=['raw_words'],sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 表示将CSV文件中每一行的第一项将填入'raw_words' field，第二项填入'target' field。\n",
    "\n",
    "# 其中项之间由'\\t'分割开来\n",
    "\n",
    "train_path=r'df_train.txt'\n",
    "\n",
    "test_path=r'df_test.txt'\n",
    "\n",
    "dataset = dataset_loader._load(train_path)\n",
    "\n",
    "testset = testset_loader._load(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39\n"
     ]
    }
   ],
   "source": [
    "# 将句子分成单词形式, 详见DataSet.apply()方法\n",
    "\n",
    "import jieba\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "print(jieba.__version__)\n",
    "# from itertools import chain\n",
    "\n",
    "#     '''\n",
    "\n",
    "#     @params:\n",
    "\n",
    "#         data: 数据的列表，列表中的每个元素为 [文本字符串，0/1标签] 二元组\n",
    "\n",
    "#     @return: 切分词后的文本的列表，列表中的每个元素为切分后的词序列\n",
    "\n",
    "#     '''\n",
    "\n",
    "def get_tokenized(data,words=True):\n",
    "    def tokenizer(text):\n",
    "        return [tok for tok in jieba.cut(text, cut_all=False)]\n",
    "    if words:\n",
    "\n",
    "        #按词语进行编码\n",
    "\n",
    "        return tokenizer(data)\n",
    "\n",
    "    else:\n",
    "\n",
    "        #按字进行编码\n",
    "\n",
    "        return [tokenizer(review) for review in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+--------+\n",
      "| raw_words                           | target |\n",
      "+-------------------------------------+--------+\n",
      "| 六一出生的？好讽刺…… //@祭春姬...   | 0      |\n",
      "| 听过一场！笑死了昂，一听茄子脱口... | 1      |\n",
      "| 说不出话来[泪] //@央广郭亮：//@...  | 0      |\n",
      "| 疼死我了，一下汗就出来了！扎我一... | 0      |\n",
      "| 凭啥不让发?!支持! //@臭丫头妈:协... | 0      |\n",
      "| 回复@PandaraShu:[哈哈][哈哈] //...  | 1      |\n",
      "| [哈哈] //@清华南都:[吐][吐]//@四... | 1      |\n",
      "| 所以新片这个造型是向自己第一部电... | 0      |\n",
      "| 就让爱已成往事吧！我们一起走过[...  | 0      |\n",
      "| [哈哈] //@日下客:回复@gerald__l...  | 1      |\n",
      "| 绝对明显  反差！还谈啥素质教育！... | 0      |\n",
      "| 回复@苏瑾coco:哇！太开心了！[爱...  | 1      |\n",
      "| 回复@凯文阿公:同意！[鼓掌] //@凯... | 1      |\n",
      "| 今晚在公司会议室跟@窦赢 用投影踢... | 1      |\n",
      "| @宁夏广电总台新闻广播 #最美宁夏...  | 1      |\n",
      "| #爸爸去哪儿威海站# 网友“尴尬美...   | 1      |\n",
      "| 哪天去尝尝 //@家有懒猫0101:@三宝... | 0      |\n",
      "| //@咱们一起去旅行:怀念哥哥！[伤...  | 0      |\n",
      "| 以后都卖切糕，保证发财。比金子都... | 0      |\n",
      "| 意大利音乐老师们说:\"学习音乐本身... | 1      |\n",
      "| 左边是晶化玻璃？//@?一堂YC: ?在...  | 0      |\n",
      "| 夜深了，忙碌一天的脚步缓和下来，... | 1      |\n",
      "| 回复@酸辣汤-说相声的张涛:[哈哈]...  | 1      |\n",
      "| 旅游，就一句话：一分钱一分货啊！... | 0      |\n",
      "| 不???的！[怒]                       | 0      |\n",
      "| 老爷一笔记本，太后一台机，两人个... | 0      |\n",
      "| [泪][怒]  //@MOON-良人：//@忆诺...  | 0      |\n",
      "| [吃惊][衰]                          | 0      |\n",
      "| 回复@l三米l:谢谢您的支持和肯定[...  | 1      |\n",
      "| 回复@爱在蔓延安德森卡卡:张北音乐... | 1      |\n",
      "| 厉害厉害！ //@莫慧兰: 我的动作给... | 1      |\n",
      "| ...                                 | ...    |\n",
      "+-------------------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.286 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------+---------------------------+\n",
      "| raw_words                 | target | words                     |\n",
      "+---------------------------+--------+---------------------------+\n",
      "| 六一出生的？好讽刺……...   | 0      | ['六一', '出生', '的'...  |\n",
      "| 听过一场！笑死了昂，一... | 1      | ['听过', '一场', '！'...  |\n",
      "| 说不出话来[泪] //@央广... | 0      | ['说不出', '话', '来'...  |\n",
      "| 疼死我了，一下汗就出来... | 0      | ['疼死', '我', '了', ...  |\n",
      "| 凭啥不让发?!支持! //@...  | 0      | ['凭', '啥', '不让', ...  |\n",
      "| 回复@PandaraShu:[哈哈...  | 1      | ['回复', '@', 'Pandar...  |\n",
      "| [哈哈] //@清华南都:[吐... | 1      | ['[', '哈哈', ']', ' ...  |\n",
      "| 所以新片这个造型是向自... | 0      | ['所以', '新片', '这个... |\n",
      "| 就让爱已成往事吧！我们... | 0      | ['就让', '爱', '已成'...  |\n",
      "| [哈哈] //@日下客:回复...  | 1      | ['[', '哈哈', ']', ' ...  |\n",
      "| 绝对明显  反差！还谈啥... | 0      | ['绝对', '明显', ' ',...  |\n",
      "| 回复@苏瑾coco:哇！太开... | 1      | ['回复', '@', '苏瑾',...  |\n",
      "| 回复@凯文阿公:同意！[...  | 1      | ['回复', '@', '凯文',...  |\n",
      "| 今晚在公司会议室跟@窦...  | 1      | ['今晚', '在', '公司'...  |\n",
      "| @宁夏广电总台新闻广播...  | 1      | ['@', '宁夏', '广电',...  |\n",
      "| #爸爸去哪儿威海站# 网...  | 1      | ['#', '爸爸', '去', '...  |\n",
      "| 哪天去尝尝 //@家有懒猫... | 0      | ['哪天', '去', '尝尝'...  |\n",
      "| //@咱们一起去旅行:怀念... | 0      | ['/', '/', '@', '咱们...  |\n",
      "| 以后都卖切糕，保证发财... | 0      | ['以后', '都', '卖', ...  |\n",
      "| 意大利音乐老师们说:\"学... | 1      | ['意大利', '音乐', '老... |\n",
      "| 左边是晶化玻璃？//@?一... | 0      | ['左边', '是', '晶化'...  |\n",
      "| 夜深了，忙碌一天的脚步... | 1      | ['夜深', '了', '，', ...  |\n",
      "| 回复@酸辣汤-说相声的张... | 1      | ['回复', '@', '酸辣汤...  |\n",
      "| 旅游，就一句话：一分钱... | 0      | ['旅游', '，', '就', ...  |\n",
      "| 不???的！[怒]             | 0      | ['不', '?', '?', '?',...  |\n",
      "| 老爷一笔记本，太后一台... | 0      | ['老爷', '一', '笔记本... |\n",
      "| [泪][怒]  //@MOON-良人... | 0      | ['[', '泪', ']', '[',...  |\n",
      "| [吃惊][衰]                | 0      | ['[', '吃惊', ']', '[...  |\n",
      "| 回复@l三米l:谢谢您的支... | 1      | ['回复', '@', 'l', '三... |\n",
      "| 回复@爱在蔓延安德森卡...  | 1      | ['回复', '@', '爱', '...  |\n",
      "| 厉害厉害！ //@莫慧兰:...  | 1      | ['厉害', '厉害', '！'...  |\n",
      "| ...                       | ...    | ...                       |\n",
      "+---------------------------+--------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "dataset.apply(lambda ins:get_tokenized(ins['raw_words']), new_field_name='words', is_input=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------+----------------------+---------+\n",
      "| raw_words            | target | words                | seq_len |\n",
      "+----------------------+--------+----------------------+---------+\n",
      "| 六一出生的？好讽...  | 0      | ['六一', '出生',...  | 42      |\n",
      "| 听过一场！笑死了...  | 1      | ['听过', '一场',...  | 48      |\n",
      "| 说不出话来[泪] /...  | 0      | ['说不出', '话',...  | 26      |\n",
      "| 疼死我了，一下汗...  | 0      | ['疼死', '我', '...  | 54      |\n",
      "| 凭啥不让发?!支持...  | 0      | ['凭', '啥', '不...  | 76      |\n",
      "| 回复@PandaraShu:...  | 1      | ['回复', '@', 'P...  | 43      |\n",
      "| [哈哈] //@清华南...  | 1      | ['[', '哈哈', ']...  | 66      |\n",
      "| 所以新片这个造型...  | 0      | ['所以', '新片',...  | 62      |\n",
      "| 就让爱已成往事吧...  | 0      | ['就让', '爱', '...  | 114     |\n",
      "| [哈哈] //@日下客...  | 1      | ['[', '哈哈', ']...  | 59      |\n",
      "| 绝对明显  反差！...  | 0      | ['绝对', '明显',...  | 61      |\n",
      "| 回复@苏瑾coco:哇...  | 1      | ['回复', '@', '苏... | 86      |\n",
      "| 回复@凯文阿公:同...  | 1      | ['回复', '@', '凯... | 43      |\n",
      "| 今晚在公司会议室...  | 1      | ['今晚', '在', '...  | 31      |\n",
      "| @宁夏广电总台新闻... | 1      | ['@', '宁夏', '广... | 82      |\n",
      "| #爸爸去哪儿威海站... | 1      | ['#', '爸爸', '去... | 98      |\n",
      "| 哪天去尝尝 //@家...  | 0      | ['哪天', '去', '...  | 51      |\n",
      "| //@咱们一起去旅行... | 0      | ['/', '/', '@', ...  | 14      |\n",
      "| 以后都卖切糕，保...  | 0      | ['以后', '都', '...  | 15      |\n",
      "| 意大利音乐老师们...  | 1      | ['意大利', '音乐...  | 83      |\n",
      "| 左边是晶化玻璃？...  | 0      | ['左边', '是', '...  | 43      |\n",
      "| 夜深了，忙碌一天...  | 1      | ['夜深', '了', '...  | 44      |\n",
      "| 回复@酸辣汤-说相...  | 1      | ['回复', '@', '酸... | 50      |\n",
      "| 旅游，就一句话：...  | 0      | ['旅游', '，', '...  | 58      |\n",
      "| 不???的！[怒]        | 0      | ['不', '?', '?',...  | 9       |\n",
      "| 老爷一笔记本，太...  | 0      | ['老爷', '一', '...  | 91      |\n",
      "| [泪][怒]  //@MOO...  | 0      | ['[', '泪', ']',...  | 44      |\n",
      "| [吃惊][衰]           | 0      | ['[', '吃惊', ']...  | 6       |\n",
      "| 回复@l三米l:谢谢...  | 1      | ['回复', '@', 'l...  | 42      |\n",
      "| 回复@爱在蔓延安德... | 1      | ['回复', '@', '爱... | 101     |\n",
      "| 厉害厉害！ //@莫...  | 1      | ['厉害', '厉害',...  | 29      |\n",
      "| ...                  | ...    | ...                  | ...     |\n",
      "+----------------------+--------+----------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "dataset.apply(lambda ins: len(ins['words']) ,new_field_name='seq_len', is_input=True)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------+----------------------+---------+\n",
      "| raw_words            | target | words                | seq_len |\n",
      "+----------------------+--------+----------------------+---------+\n",
      "| 六一出生的？好讽...  | 0      | ['六一', '出生',...  | 42      |\n",
      "| 听过一场！笑死了...  | 1      | ['听过', '一场',...  | 48      |\n",
      "| 说不出话来[泪] /...  | 0      | ['说不出', '话',...  | 26      |\n",
      "| 疼死我了，一下汗...  | 0      | ['疼死', '我', '...  | 54      |\n",
      "| 凭啥不让发?!支持...  | 0      | ['凭', '啥', '不...  | 76      |\n",
      "| 回复@PandaraShu:...  | 1      | ['回复', '@', 'P...  | 43      |\n",
      "| [哈哈] //@清华南...  | 1      | ['[', '哈哈', ']...  | 66      |\n",
      "| 所以新片这个造型...  | 0      | ['所以', '新片',...  | 62      |\n",
      "| 就让爱已成往事吧...  | 0      | ['就让', '爱', '...  | 114     |\n",
      "| [哈哈] //@日下客...  | 1      | ['[', '哈哈', ']...  | 59      |\n",
      "| 绝对明显  反差！...  | 0      | ['绝对', '明显',...  | 61      |\n",
      "| 回复@苏瑾coco:哇...  | 1      | ['回复', '@', '苏... | 86      |\n",
      "| 回复@凯文阿公:同...  | 1      | ['回复', '@', '凯... | 43      |\n",
      "| 今晚在公司会议室...  | 1      | ['今晚', '在', '...  | 31      |\n",
      "| @宁夏广电总台新闻... | 1      | ['@', '宁夏', '广... | 82      |\n",
      "| #爸爸去哪儿威海站... | 1      | ['#', '爸爸', '去... | 98      |\n",
      "| 哪天去尝尝 //@家...  | 0      | ['哪天', '去', '...  | 51      |\n",
      "| //@咱们一起去旅行... | 0      | ['/', '/', '@', ...  | 14      |\n",
      "| 以后都卖切糕，保...  | 0      | ['以后', '都', '...  | 15      |\n",
      "| 意大利音乐老师们...  | 1      | ['意大利', '音乐...  | 83      |\n",
      "| 左边是晶化玻璃？...  | 0      | ['左边', '是', '...  | 43      |\n",
      "| 夜深了，忙碌一天...  | 1      | ['夜深', '了', '...  | 44      |\n",
      "| 回复@酸辣汤-说相...  | 1      | ['回复', '@', '酸... | 50      |\n",
      "| 旅游，就一句话：...  | 0      | ['旅游', '，', '...  | 58      |\n",
      "| 不???的！[怒]        | 0      | ['不', '?', '?',...  | 9       |\n",
      "| 老爷一笔记本，太...  | 0      | ['老爷', '一', '...  | 91      |\n",
      "| [泪][怒]  //@MOO...  | 0      | ['[', '泪', ']',...  | 44      |\n",
      "| [吃惊][衰]           | 0      | ['[', '吃惊', ']...  | 6       |\n",
      "| 回复@l三米l:谢谢...  | 1      | ['回复', '@', 'l...  | 42      |\n",
      "| 回复@爱在蔓延安德... | 1      | ['回复', '@', '爱... | 101     |\n",
      "| 厉害厉害！ //@莫...  | 1      | ['厉害', '厉害',...  | 29      |\n",
      "| ...                  | ...    | ...                  | ...     |\n",
      "+----------------------+--------+----------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "dataset.apply(lambda x: int(x['target']), new_field_name='target', is_target=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------------------------+---------+\n",
      "| raw_words                 | words                     | seq_len |\n",
      "+---------------------------+---------------------------+---------+\n",
      "| 就是累了，比舒小姐要早... | ['就是', '累', '了', ...  | 82      |\n",
      "| 今天紫外线强度太强了，... | ['今天', '紫外线', '强... | 20      |\n",
      "| ~~ //@lichgo://@雪虹坚... | ['~', '~', ' ', '/', ...  | 118     |\n",
      "| 有一个小姑娘，她有两部... | ['有', '一个', '小姑娘... | 75      |\n",
      "| 感动[泪][泪]              | ['感动', '[', '泪', '...  | 7       |\n",
      "| 早起，坐公交，吹冷风。... | ['早起', '，', '坐', ...  | 26      |\n",
      "| 北京冬季天气偏干燥，及... | ['北京', '冬季', '天气... | 32      |\n",
      "| 哈哈哈哈[哈哈]//@鹦鹉...  | ['哈哈哈哈', '[', '哈...  | 24      |\n",
      "| 回复@暗地黑羊:别打人，... | ['回复', '@', '暗地',...  | 108     |\n",
      "| 还记得小时候看过一种说... | ['还', '记得', '小时候... | 33      |\n",
      "| 太客气啦啦....么么哒！... | ['太', '客气', '啦', ...  | 20      |\n",
      "| TVB新片?拜兼?年?[鼓掌...  | ['TVB', '新片', '?', ...  | 11      |\n",
      "| 跳了么，你们跳了么？/...  | ['跳', '了', '么', '，... | 32      |\n",
      "| [泪] //@梦都夜话之风之... | ['[', '泪', ']', ' ',...  | 53      |\n",
      "| 刚接到店家的通知，今年... | ['刚', '接到', '店家'...  | 88      |\n",
      "| 可恶的生物钟，我要睡觉... | ['可恶', '的', '生物钟... | 12      |\n",
      "| 我的钱就是这么木有地[...  | ['我', '的', '钱', '就... | 10      |\n",
      "| 回复@海?日?:小心[偷笑...  | ['回复', '@', '海', '...  | 133     |\n",
      "| 笑尿了[嘻嘻]//@鸵鸟小...  | ['笑', '尿', '了', '[...  | 40      |\n",
      "| [抓狂][抓狂][抓狂][抓...  | ['[', '抓狂', ']', '[...  | 32      |\n",
      "| 还需要社会主义贤内助[...  | ['还', '需要', '社会主... | 50      |\n",
      "| [嘻嘻]@联众扑克世界 的... | ['[', '嘻嘻', ']', '@...  | 38      |\n",
      "| 成熟不是心变老,而是心...  | ['成熟', '不是', '心变... | 54      |\n",
      "| 骄傲。 //@Eline艳艳:[...  | ['骄傲', '。', ' ', '...  | 52      |\n",
      "| 配图很喜感[哈哈]          | ['配图', '很', '喜感'...  | 6       |\n",
      "| #陶言无忌#午餐时，和老... | ['#', '陶', '言无忌',...  | 88      |\n",
      "| 要不要这么给我添乱啊，... | ['要', '不要', '这么'...  | 20      |\n",
      "| 【你是泉州土著吗？】鉴... | ['【', '你', '是', '泉... | 24      |\n",
      "| 不客?, 等??用後分享心...  | ['不客', '?', ',', ' ...  | 42      |\n",
      "| 梁静茹，可惜不是你[哈...  | ['梁静茹', '，', '可惜... | 26      |\n",
      "| 哈哈 终于有所斩获。这...  | ['哈哈', ' ', '终于',...  | 14      |\n",
      "| ...                       | ...                       | ...     |\n",
      "+---------------------------+---------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "#testset.apply(lambda ins: list(chain.from_iterable(get_tokenized(ins['raw_words']))), new_field_name='words', is_input=True)\n",
    "\n",
    "testset.apply(lambda ins: get_tokenized(ins['raw_words']), new_field_name='words', is_input=True)\n",
    "\n",
    "testset.apply(lambda ins: len(ins['words']) ,new_field_name='seq_len',is_input=True)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "from fastNLP import Vocabulary\n",
    "\n",
    "#将DataSet按照ratio的比例拆分，返回两个DataSet\n",
    "\n",
    "#ratio (float) -- 0<ratio<1, 返回的第一个DataSet拥有 (1-ratio) 这么多数据，第二个DataSet拥有`ratio`这么多数据\n",
    "\n",
    "train_data, dev_data = dataset.split(0.1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------+----------------------+---------+\n",
      "| raw_words            | target | words                | seq_len |\n",
      "+----------------------+--------+----------------------+---------+\n",
      "| 打喷嚏，流鼻涕，...  | 0      | ['打喷嚏', '，',...  | 20      |\n",
      "| 今天去南京为新剧...  | 0      | ['今天', '去', '...  | 97      |\n",
      "| 路上都有过年的气...  | 0      | ['路上', '都', '...  | 25      |\n",
      "| 回复@张翡翠:嘿嘿...  | 1      | ['回复', '@', '张... | 95      |\n",
      "| 国外的一些无节操...  | 1      | ['国外', '的', '...  | 16      |\n",
      "| 中奖几率会更高吗...  | 1      | ['中奖', '几率',...  | 63      |\n",
      "| 怎么就知道拍照片...  | 0      | ['怎么', '就', '...  | 14      |\n",
      "| #豆果年会#超好玩...  | 1      | ['#', '豆果', '年... | 53      |\n",
      "| //@微博搞笑排行榜... | 1      | ['/', '/', '@', ...  | 23      |\n",
      "| 这里明明都是高富...  | 1      | ['这里', '明明',...  | 8       |\n",
      "| [衰]//@爱做饭的疯... | 0      | ['[', '衰', ']',...  | 15      |\n",
      "| 过度解读！！[悲伤... | 0      | ['过度', '解读',...  | 36      |\n",
      "| //@百合网: 最后半... | 1      | ['/', '/', '@', ...  | 26      |\n",
      "| 别再问我为什么今...  | 0      | ['别', '再', '问...  | 53      |\n",
      "| 感谢您的支持[嘻嘻... | 1      | ['感谢您', '的',...  | 6       |\n",
      "| 回复@加州紫茉莉:...  | 1      | ['回复', '@', '加... | 82      |\n",
      "| 晚饭时间到好饿[泪... | 0      | ['晚饭时间', '到...  | 25      |\n",
      "| 2014第一天就大吉...  | 0      | ['2014', '第一天...  | 39      |\n",
      "| 不举不行、老举也...  | 1      | ['不', '举', '不...  | 16      |\n",
      "| //@满满笑容的春儿... | 0      | ['/', '/', '@', ...  | 67      |\n",
      "| WOW这个太心水了[...  | 1      | ['WOW', '这个', ...  | 51      |\n",
      "| 愚人节最头疼的就...  | 0      | ['愚人节', '最',...  | 12      |\n",
      "| 惊现俺家少卿姐，...  | 1      | ['惊现', '俺家',...  | 53      |\n",
      "| #三亚旅游#直播同...  | 1      | ['#', '三亚旅游'...  | 31      |\n",
      "| 回复@蓝色海洋:海...  | 1      | ['回复', '@', '蓝... | 87      |\n",
      "| 我激动了！！！[鼓... | 1      | ['我', '激动', '...  | 21      |\n",
      "| 我靠!怎么这么大的... | 0      | ['我', '靠', '!'...  | 36      |\n",
      "| 回复@黑米粒:抱抱...  | 1      | ['回复', '@', '黑... | 49      |\n",
      "| //@潘洪其:求屁咬...  | 0      | ['/', '/', '@', ...  | 42      |\n",
      "| 爱旅行的帅哥，来...  | 1      | ['爱', '旅行', '...  | 16      |\n",
      "| 【外姐在京城】天...  | 0      | ['【', '外姐', '...  | 88      |\n",
      "| ...                  | ...    | ...                  | ...     |\n",
      "+----------------------+--------+----------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98990 10998 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data),len(dev_data),len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary(['六一', '出生', '的', '？', '好']...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary(min_freq=2).from_dataset(dataset, field_name='words')\n",
    "\n",
    "vocab.index_dataset(train_data, dev_data, testset, field_name='words', new_field_name='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 out of 93812 words in the pre-training embedding.\n"
     ]
    }
   ],
   "source": [
    "from fastNLP.embeddings import StaticEmbedding,StackEmbedding\n",
    "\n",
    "fastnlp_embed = StaticEmbedding(vocab, model_dir_or_name='cn-char-fastnlp-100d',min_freq=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNText(\n",
      "  (embed): Embedding(\n",
      "    (embed): StaticEmbedding(\n",
      "      (dropout_layer): Dropout(p=0)\n",
      "      (embedding): Embedding(93812, 100, padding_idx=0)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0)\n",
      "  )\n",
      "  (conv_pool): ConvMaxpool(\n",
      "    (convs): ModuleList(\n",
      "      (0): Conv1d(100, 30, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (1): Conv1d(100, 40, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (2): Conv1d(100, 50, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (fc): Linear(in_features=120, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from fastNLP.models import CNNText\n",
    "\n",
    "model_CNN = CNNText(fastnlp_embed, num_classes=2,dropout=0.1)\n",
    "\n",
    "print(model_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input fields after batch(if batch size is 2):\n",
      "\twords: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 97]) \n",
      "\tseq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2020-06-18-18-00-50-035273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c562d28b2c5b4d13864fbc52cd1138a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=30940.0), HTML(value='')), layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=344.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 14.29 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 1/10. Step:3094/30940: \n",
      "\r",
      "AccuracyMetric: acc=0.985452\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=344.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 13.89 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 2/10. Step:6188/30940: \n",
      "\r",
      "AccuracyMetric: acc=0.959083\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=344.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 14.57 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 3/10. Step:9282/30940: \n",
      "\r",
      "AccuracyMetric: acc=0.985543\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2729dc436bd7410b9bef211891f9d282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=344.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastNLP import Trainer, CrossEntropyLoss, AccuracyMetric,BCELoss\n",
    "\n",
    "trainer_CNN = Trainer(model=model_CNN, train_data=train_data, dev_data=dev_data,loss=CrossEntropyLoss(), metrics=AccuracyMetric())\n",
    "\n",
    "trainer_CNN.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "def batch_predict(model,data):\n",
    "#     submission=pd.DataFrame(columns=['Prediction'])\n",
    "#    submission = pd.DataFrame(columns=['ID','Prediction'])\n",
    "    demo1=[]\n",
    "    for i in range(len(data)):\n",
    "    #for i in range(5):\n",
    "#         print(data.words[i])\n",
    "        tensor = torch.tensor(data.words[i])\n",
    "        pred = model.predict(tensor.view(1,-1))\n",
    "#         print(pred)\n",
    "        prob = pred['pred'].numpy()[0]\n",
    "#         print(\"pred:%.2f\"%(prob))\n",
    "#         print('='*50)\n",
    "#         print(type(prob))\n",
    "#         s2 = pd.Series([float(prob)], index=['Prediction'])\n",
    "        demo1.append(prob)\n",
    "#         print(prob)\n",
    "# #         print(s2)\n",
    "#         submission = submission.append(s2, ignore_index=True)\n",
    "#         submission['Prediction'] = submission.Prediction .astype(int)\n",
    "#         submission['']\n",
    "#         submission['Prediction'] = submission.Prediction.astype(float) \n",
    "    return demo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo=batch_predict(model_CNN,testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(demo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(testlist,demo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
